{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline d‚Äôanalyse pr√©dictive : de l‚Äôexploration des donn√©es √† l‚Äô√©valuation du mod√®le\n"
      ],
      "metadata": {
        "id": "5HXM2hH78mk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LOM--fh1psQ",
        "outputId": "782c28a2-ab4e-46b7-945c-c698d5046508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import des biblioth√®ques\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Mod√®les\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "#from catboost import CatBoostClassifier\n",
        "\n",
        "# M√©triques\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, f1_score, accuracy_score, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Optimisation\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqMOBGDN9Lv2",
        "outputId": "ba4f6e98-5a49-45e2-96ca-34fd23a6ab50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Biblioth√®ques import√©es avec succ√®s!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pr√©dire l'acc√®s √† la fibre optique au Togo : Retour d'exp√©rience technique complet\n",
        "\n",
        "## Comment j'ai atteint 91% AUC avec Random Forest sur 30K m√©nages et 4000+ features\n",
        "\n",
        "*Article technique approfondi - 15 min de lecture*\n",
        "\n",
        "**Par : [Votre Nom] | Consultant Data Science sp√©cialis√© en projets d'impact social**\n"
      ],
      "metadata": {
        "id": "cAUnZh4K12-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le D√©fi\n",
        "\n",
        "Imaginez devoir pr√©dire quels m√©nages au Togo adopteront la fibre optique (FTTH), avec :\n",
        "-  **30 558 observations** √ó **4043 colonnes** (dont 4000 features MOSAIKS)\n",
        "-  Des donn√©es h√©t√©rog√®nes (recensement + satellites)\n",
        "-  Un territoire √† g√©om√©trie variable (urbain/rural)\n",
        "-  Des contraintes de d√©ploiement r√©elles (budget, ROI)\n",
        "- Mais une mission claire : **r√©duire la fracture num√©rique**\n",
        "\n",
        "**Spoiler** : Le Random Forest a atteint **AUC = 0.907** (certains runs √† 0.909), mais le vrai apprentissage est dans le **preprocessing** et le **pipeline design**.\n"
      ],
      "metadata": {
        "id": "D22MfayP2Vyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. **Dataset : Anatomie d'un jeu de donn√©es complexe**\n",
        "\n",
        "### Sources de donn√©es multiples\n",
        "\n",
        "```\n",
        "Dataset Final : 30 558 observations √ó 4043 colonnes\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ üìã RGPH (Recensement Population & Habitat - INSEED Togo)\n",
        "‚îÇ   ‚îú‚îÄ 42 variables socio-d√©mographiques\n",
        "‚îÇ   ‚îú‚îÄ Taille m√©nage, type logement, √©quipements\n",
        "‚îÇ   ‚îî‚îÄ Variables cat√©gorielles (TypeLogmt_1/2/3, H17A-J, H18A-J, etc.)\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ üõ∞Ô∏è MOSAIKS (Berkeley - Multi-task Observation using Satellite Imagery & Kitchen Sinks)\n",
        "‚îÇ   ‚îú‚îÄ 4001 features extraites automatiquement d'images satellites\n",
        "‚îÇ   ‚îú‚îÄ Caract√©ristiques g√©ospatiales : texture urbaine, densit√© b√¢ti, v√©g√©tation\n",
        "‚îÇ   ‚îî‚îÄ Variables continues hautement corr√©l√©es (multicolin√©arit√© ++++)\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ üì° Op√©rateurs (Togocom / GVA)\n",
        "    ‚îú‚îÄ Variable cible binaire : \"Acc√®s internet\" (0/1)\n",
        "    ‚îú‚îÄ Distribution : 51.2% sans acc√®s | 48.8% avec acc√®s\n",
        "    ‚îî‚îÄ  Dataset relativement √©quilibr√© (pas de SMOTE agressif n√©cessaire)\n",
        "```"
      ],
      "metadata": {
        "id": "o4YQ-uNM28km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des donn√©es\n",
        "# Remplacez 'votre_fichier.csv' par le chemin de votre fichier de donn√©es\n",
        "df = pd.read_csv('/content/drive/MyDrive/Project ACDS-20251216T101021Z-1-002/Project ACDS/Data_001.csv')\n",
        "\n",
        "print(f\" Dimension du dataset: {df.shape}\")\n",
        "print(f\" Nombre de lignes: {df.shape[0]:,}\")\n",
        "print(f\" Nombre de colonnes: {df.shape[1]:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj5NpJcd8yCT",
        "outputId": "21300e05-aed7-4f44-ab38-627d717cc396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dimension du dataset: (30558, 4043)\n",
            " Nombre de lignes: 30,558\n",
            " Nombre de colonnes: 4,043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. **Preprocessing : Le 80% invisible du travail**\n",
        "\n",
        " ### 2.1. Identifier les features MOSAIKS : Challenge #1\n",
        "\n",
        "**Probl√®me** : 4043 colonnes m√©lang√©es (socio-√©conomiques + satellites). Comment les distinguer automatiquement ?\n",
        "\n",
        "**Solution : Heuristique statistique**\n"
      ],
      "metadata": {
        "id": "Zs43spQt5Hn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_mosaiks_features(df, threshold=50):\n",
        "    \"\"\"\n",
        "    D√©tecte automatiquement les features MOSAIKS :\n",
        "    - Variables continues (float64)\n",
        "    - Haute cardinalit√© (> 50 valeurs uniques)\n",
        "    - Pas de valeurs manquantes (d√©j√† imput√©es)\n",
        "    \"\"\"\n",
        "    mosaiks_cols = []\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['float64', 'float32']:\n",
        "            if df[col].nunique() > threshold:\n",
        "                mosaiks_cols.append(col)\n",
        "    return mosaiks_cols\n",
        "\n",
        "# R√©sultat : 4001 features MOSAIKS identifi√©es\n",
        "\n",
        "# Cette approche automatique √©vite le hard-coding des noms de colonnes. R√©utilisable sur d'autres datasets g√©ospatiaux.\n"
      ],
      "metadata": {
        "id": "RHPMcblU5KCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2.2. Colonnes constantes et redondantes : Challenge #2\n",
        "\n",
        "**Probl√®me** : Certaines colonnes ont une seule valeur unique ‚Üí bruit inutile.\n"
      ],
      "metadata": {
        "id": "t5IuveJ75ef_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identification des colonnes constantes\n",
        "constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "\n",
        "print(f\"Colonnes constantes d√©tect√©es : {len(constant_cols)}\")\n",
        "# R√©sultat : 35 colonnes (ex: 'BoxLabel', '.184', '.240', etc.)\n",
        "\n",
        "# Suppression\n",
        "df.drop(columns=constant_cols, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYc5WUG15k1A",
        "outputId": "6ecb43e5-2971-46ee-a5ab-398942f88f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonnes constantes d√©tect√©es : 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impact** :\n",
        "- R√©duction dimensionnelle de 4043 ‚Üí 4008 colonnes\n",
        "- Acc√©l√©ration entra√Ænement de ~5%\n",
        "- Moins de risque d'overfitting\n"
      ],
      "metadata": {
        "id": "_32iH7vH5qDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2.3. Valeurs manquantes : Challenge #3\n",
        "\n",
        "> Ajouter une citation\n",
        "\n",
        "\n",
        "\n",
        "**Analyse** :\n"
      ],
      "metadata": {
        "id": "g3bLXDpg51HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# V√©rification des colonnes avec > 60% de valeurs manquantes\n",
        "missing_threshold = 0.60\n",
        "high_missing = df.isnull().sum() / len(df)\n",
        "high_missing_cols = high_missing[high_missing > missing_threshold]\n",
        "\n",
        "print(f\"Colonnes avec > 60% manquantes : {len(high_missing_cols)}\")\n",
        "# R√©sultat : 0 colonnes (dataset d√©j√† bien nettoy√© par l'op√©rateur)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrhwmccS55L5",
        "outputId": "63172490-1752-4e05-d397-2776677b7463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonnes avec > 60% manquantes : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "# Pour les variables num√©riques : imputation par la m√©diane (robuste aux outliers)\n",
        "for col in numeric_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# Pour les cat√©gorielles : mode ou label 'inconnu'\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "'''\n",
        "** Pourquoi la m√©diane ?**\n",
        "- Plus robuste que la moyenne face aux outliers\n",
        "- Pr√©serve la distribution originale\n",
        "- √âvite d'introduire des valeurs irr√©alistes\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "I9ZMT87b-LXg",
        "outputId": "84f7ed9e-a297-45c1-d122-95dcad7af77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n** Pourquoi la m√©diane ?**\\n- Plus robuste que la moyenne face aux outliers\\n- Pr√©serve la distribution originale\\n- √âvite d'introduire des valeurs irr√©alistes\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###  2.4. Encodage des variables cat√©gorielles : Challenge #4\n",
        "\n",
        "**Contexte** : 40 variables cat√©gorielles (TypeLogmt, H17A-J, H18A-J, Connexion, etc.)\n",
        "\n",
        "**3 strat√©gies test√©es** :\n"
      ],
      "metadata": {
        "id": "HE5qUafu-wAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  2.4.1. LabelEncoder (utilis√© ici)"
      ],
      "metadata": {
        "id": "p7tDnaq4-69M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le  # Sauvegarde pour inverse_transform"
      ],
      "metadata": {
        "id": "jk6J12q6-_Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Avantages** :\n",
        "-  Rapide et peu co√ªteux en m√©moire\n",
        "-  Compatible Random Forest (g√®re l'ordinalit√© implicite)\n",
        "\n",
        "**Inconv√©nients** :\n",
        "-  Introduit un ordre artificiel (ex: TypeLogmt_1=0, TypeLogmt_2=1, TypeLogmt_3=2)\n",
        "-  Probl√©matique pour les mod√®les lin√©aires (Logistic Regression)"
      ],
      "metadata": {
        "id": "jbqb1sYJ_GYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####   2.4.2. OneHotEncoder (alternative test√©e)"
      ],
      "metadata": {
        "id": "7bXWRFEN_Ijx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "encoded = encoder.fit_transform(df[categorical_cols])\n",
        "\n",
        "# Probl√®me : explosion dimensionnelle\n",
        "# 40 colonnes cat√©gorielles ‚Üí 150-200 colonnes after OHE"
      ],
      "metadata": {
        "id": "XFC_LpMa_OUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Verdict** : Abandonn√© car :\n",
        "- Augmente encore plus la dimensionnalit√© (4000 ‚Üí 4200+ colonnes)\n",
        "- Random Forest n'a pas besoin d'OHE\n"
      ],
      "metadata": {
        "id": "Zw0Injs__l_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####   2.4.3. ColumnTransformer (approche mixte - recommand√©e)"
      ],
      "metadata": {
        "id": "X0YcPTPv_yJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
        "    (\"num\", StandardScaler(), numeric_cols)\n",
        "])\n",
        "\n",
        "# Int√©gration dans un Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "ZXYUnjBM_x2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Meilleure pratique** : Utiliser `ColumnTransformer` + `Pipeline` pour :\n",
        "- √âviter le data leakage (fit sur train, transform sur test)\n",
        "- Assurer la reproductibilit√©\n",
        "- D√©ploiement simplifi√© (un seul objet .pkl)\n"
      ],
      "metadata": {
        "id": "5MhHu95J_-9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###   2.5. Normalisation (StandardScaler) : Challenge #5\n",
        "\n",
        "**Pourquoi normaliser ?**\n"
      ],
      "metadata": {
        "id": "EIuqHAgjAZvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avant normalisation\n",
        "print(df['TAILLE_MENAGE'].describe())\n",
        "# mean: 4.47, std: 2.89, min: 1, max: 25\n",
        "\n",
        "print(df['.0'].describe())  # Feature MOSAIKS\n",
        "# mean: 0.00034, std: 0.00012, min: -0.0005, max: 0.0015"
      ],
      "metadata": {
        "id": "DJNjZf_MAZcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**√âchelles tr√®s diff√©rentes** ‚Üí Probl√®me pour :\n",
        "- PCA (dominance des grandes valeurs)\n",
        "- Mod√®les lin√©aires (poids biais√©s)\n",
        "- Calcul de distances\n",
        "\n",
        "**Solution** :\n"
      ],
      "metadata": {
        "id": "i-C5w4PGtZVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# R√©sultat : mean ‚âà 0, std ‚âà 1 pour toutes les features"
      ],
      "metadata": {
        "id": "plKDFXoPtdWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3. **Feature Engineering : De 4000 √† 72 features intelligentes**\n",
        "\n",
        "###   3.1. PCA sur les features MOSAIKS\n",
        "\n",
        "**Probl√®me** : 4001 features MOSAIKS ‚Üí Multicolin√©arit√© extr√™me + Overfitting\n",
        "\n",
        "**Solution : PCA (Principal Component Analysis)**\n"
      ],
      "metadata": {
        "id": "eqgMFz3Nti3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Extraction des features MOSAIKS\n",
        "mosaiks_features = df[mosaiks_cols]\n",
        "\n",
        "# Normalisation AVANT PCA (obligatoire)\n",
        "scaler_mosaiks = StandardScaler()\n",
        "mosaiks_scaled = scaler_mosaiks.fit_transform(mosaiks_features)\n",
        "\n",
        "# PCA : r√©duction √† 30 composantes\n",
        "pca = PCA(n_components=30, random_state=45)\n",
        "mosaiks_pca = pca.fit_transform(mosaiks_scaled)\n",
        "\n",
        "# Conversion en DataFrame\n",
        "pca_df = pd.DataFrame(\n",
        "    mosaiks_pca,\n",
        "    columns=[f'PCA_MOSAIKS_{i}' for i in range(30)]\n",
        ")\n",
        "\n",
        "print(f\"Variance expliqu√©e : {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "# R√©sultat : 0.9970 (99.7% de variance conserv√©e !)"
      ],
      "metadata": {
        "id": "XinPgQu1tuK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualisation de la variance expliqu√©e** :"
      ],
      "metadata": {
        "id": "uiv9RHujt2P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Graphique 1 : Variance par composante\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, 31), pca.explained_variance_ratio_, 'bo-')\n",
        "plt.xlabel('Composante Principale')\n",
        "plt.ylabel('Variance Expliqu√©e')\n",
        "plt.title('Variance par Composante')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Graphique 2 : Variance cumul√©e\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, 31), np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
        "plt.axhline(y=0.95, color='g', linestyle='--', label='95% variance')\n",
        "plt.axhline(y=0.997, color='b', linestyle='--', label='99.7% variance')\n",
        "plt.xlabel('Nombre de Composantes')\n",
        "plt.ylabel('Variance Cumul√©e')\n",
        "plt.title('Variance Cumul√©e')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jZwHsuK1t2Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**D√©couverte cl√©** :\n",
        "- Les **10 premi√®res composantes** capturent d√©j√† **95% de la variance**\n",
        "- Les 20 derni√®res composantes ajoutent seulement 2%\n",
        "- On aurait pu descendre √† 15-20 composantes sans perte significative\n"
      ],
      "metadata": {
        "id": "0Ev-2sWyt78k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###   3.2. S√©lection des features socio-√©conomiques"
      ],
      "metadata": {
        "id": "nf72fENLh1IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Probl√®me** : 42 variables socio-√©conomiques, mais toutes ne sont pas pertinentes.\n",
        "\n",
        "**Solution : SelectKBest avec Mutual Information**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Extraction des features socio-√©conomiques (d√©j√† encod√©es)\n",
        "socio_features = df[socio_cols]\n",
        "\n",
        "# S√©lection des k meilleures features\n",
        "k_socio = 42  # Commencer avec toutes, puis r√©duire\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=k_socio)\n",
        "selector.fit(socio_features, y_train)\n",
        "\n",
        "# Ranking des features par importance\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': socio_cols,\n",
        "    'Score': selector.scores_\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(feature_scores.head(10))\n"
      ],
      "metadata": {
        "id": "uuamblnyuG9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**R√©sultats top 10** :\n",
        "\n",
        "| Feature | Mutual Info Score |\n",
        "||-|\n",
        "| Connexion | 0.4512 |\n",
        "| TypeLogmt_3 | 0.0892 |\n",
        "| TAILLE_MENAGE | 0.0645 |\n",
        "| H18E (√âquipement) | 0.0423 |\n",
        "| H08_Impute | 0.0389 |\n",
        "| TypeLogmt_1 | 0.0301 |\n",
        "| H20A | 0.0267 |\n",
        "| H17B | 0.0234 |\n",
        "| H09_Impute | 0.0198 |\n",
        "| H18A | 0.0176 |\n",
        "\n",
        "**Insight** : La variable `Connexion` (infrastructure existante) domine avec 45% d'information mutuelle. C'est le **pr√©dicteur le plus puissant**.\n"
      ],
      "metadata": {
        "id": "ayR9LXMSuLto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Feature Importance avec LightGBM\n",
        "\n",
        "**Approche alternative** : Entra√Æner un LightGBM rapide pour estimer l'importance.\n"
      ],
      "metadata": {
        "id": "QJqQfzr9ulpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Fusion PCA + Socio\n",
        "X_combined = pd.concat([pca_df, socio_features], axis=1)\n",
        "\n",
        "# Mod√®le LightGBM rapide\n",
        "lgbm_selector = LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=45,\n",
        "    verbose=-1\n",
        ")\n",
        "lgbm_selector.fit(X_combined, y_train)\n",
        "\n",
        "# Extraction des importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_combined.columns,\n",
        "    'Importance': lgbm_selector.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# S√©lection des 100 features les plus importantes\n",
        "top_100_features = feature_importances.head(100)['Feature'].tolist()\n",
        "X_final = X_combined[top_100_features]\n",
        "\n",
        "print(f\"R√©duction : {X_combined.shape[1]} ‚Üí {len(top_100_features)} features\")\n",
        "# R√©sultat : 72 ‚Üí 100 features (ou moins selon le seuil)"
      ],
      "metadata": {
        "id": "PDZoGKH7ulMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Astuce** : Cette approche est **plus rapide** que des m√©thodes exhaustives (RFE) et donne de bons r√©sultats en pratique."
      ],
      "metadata": {
        "id": "eDoaaa0suunh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.4. Dataset final apr√®s feature engineering\n",
        "\n",
        "```\n",
        "Dataset Final : 30 558 observations √ó 72 colonnes\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ 30 composantes PCA (MOSAIKS compress√©es)\n",
        "‚îú‚îÄ‚îÄ 42 variables socio-√©conomiques (encod√©es)\n",
        "‚îî‚îÄ‚îÄ 1 variable cible (Acc√®s internet)\n",
        "\n",
        "R√©duction dimensionnelle : 4043 ‚Üí 72 (-98.2%)\n",
        "Variance MOSAIKS conserv√©e : 99.7%\n",
        "Information socio-√©conomique : 100% (toutes gard√©es)\n",
        "```\n"
      ],
      "metadata": {
        "id": "c5Gu16QWu1Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. **Mod√©lisation : Comparaison syst√©matique**\n",
        "\n",
        "### M√©thodologie rigoureuse"
      ],
      "metadata": {
        "id": "bOU5Suwsu5qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train/Test Split stratifi√©\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, y,\n",
        "    test_size=0.20,\n",
        "    random_state=45,  # Reproductibilit√©\n",
        "    stratify=y         # Pr√©serve la distribution 51:49\n",
        ")\n",
        "\n",
        "print(f\"Train set : {X_train.shape}\")  # (24 446, 72)\n",
        "print(f\"Test set  : {X_test.shape}\")   # (6 112, 72)\n",
        "\n",
        "print(f\"\\nDistribution y_train:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "# 0    0.512\n",
        "# 1    0.488"
      ],
      "metadata": {
        "id": "45cTZMtau-zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.1. Mod√®les test√©s et r√©sultats\n",
        "\n",
        "#### 4.1.1Ô∏è. Logistic Regression (Baseline)"
      ],
      "metadata": {
        "id": "OTbBNRZsvMxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pourquoi random_state=45 ?**\n",
        "- Permet la **reproductibilit√© exacte** des r√©sultats\n",
        "- Facilite le debugging et la comparaison d'exp√©riences\n",
        "- Convention : utiliser le m√™me random_state partout (PCA, KMeans, split, mod√®les)\n"
      ],
      "metadata": {
        "id": "b62cAdDxvEQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    max_iter=500,\n",
        "    random_state=45,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# R√©sultats\n",
        "# Accuracy : 81.7%\n",
        "# F1-Score : 83.5%\n",
        "# AUC      : 0.853"
      ],
      "metadata": {
        "id": "stmzJ04MvXRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyse** :\n",
        "-  Rapide √† entra√Æner (< 5 sec)\n",
        "-  Interpr√©table (coefficients)\n",
        "-  Performance limit√©e (AUC 0.85)\n",
        "-  Suppose des relations lin√©aires (faux ici)\n"
      ],
      "metadata": {
        "id": "JC19Srlxva8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.1.2Ô∏è. Random Forest (Champion)\n"
      ],
      "metadata": {
        "id": "fKpXQYG_vmpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=45,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# R√©sultats\n",
        "# Accuracy : 86.4%\n",
        "# F1-Score : 87.3%\n",
        "# AUC      : 0.907"
      ],
      "metadata": {
        "id": "Ehl9R1zUvmE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Hyperparam√®tres optimaux** :\n",
        "- `n_estimators=300` : Bon compromis performance/temps (vs 500)\n",
        "- `max_depth=20` : Profondeur suffisante sans overfitting\n",
        "- `min_samples_split=5` : Emp√™che les arbres trop sp√©cifiques\n",
        "- `class_weight='balanced'` : Compense le l√©ger d√©s√©quilibre\n",
        "\n",
        "**Pourquoi Random Forest gagne ?**\n",
        "1. **Robustesse aux corr√©lations** : Les 72 features (dont 30 PCA) ont des corr√©lations r√©siduelles. RF les g√®re mieux que les mod√®les lin√©aires.\n",
        "2. **Capture des interactions** : Relations complexes entre `zone_geographique` √ó `taille_menage` √ó `connexion`.\n",
        "3. **G√©n√©ralisation spatiale** : Moins d'overfitting que XGBoost/LightGBM sur les clusters g√©ographiques.\n"
      ],
      "metadata": {
        "id": "AaadyOIjv1IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3Ô∏è. LightGBM (Runner-up)"
      ],
      "metadata": {
        "id": "i9drN7Cpv-l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    num_leaves=64,\n",
        "    max_depth=12,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=45,\n",
        "    verbose=-1\n",
        ")\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "# R√©sultats\n",
        "# Accuracy : 86.2%\n",
        "# F1-Score : 87.3%\n",
        "# AUC      : 0.909  ü•à (sur certains runs)"
      ],
      "metadata": {
        "id": "sLGZf2aTv00j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyse** :\n",
        "-  Tr√®s rapide (~3 sec vs 15 sec pour RF)\n",
        "-  Performances similaires voire l√©g√®rement sup√©rieures\n",
        "-  Plus sensible aux hyperparam√®tres\n",
        "-  Risque d'overfitting si mal r√©gl√©\n",
        "\n",
        "** Quand pr√©f√©rer LightGBM ?**\n",
        "- Datasets tr√®s larges (> 100K lignes)\n",
        "- Contraintes de temps d'entra√Ænement\n",
        "- Comp√©titions Kaggle (optimisation pouss√©e)\n"
      ],
      "metadata": {
        "id": "FUSIZh0DxCjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.1.4Ô∏è. XGBoost"
      ],
      "metadata": {
        "id": "Ta1u85Wkw9Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=10,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=45\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# R√©sultats\n",
        "# Accuracy : 83.8%\n",
        "# F1-Score : 85.5%\n",
        "# AUC      : 0.873"
      ],
      "metadata": {
        "id": "asnYhq1nxIo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Analyse** :\n",
        "-  Solide et stable\n",
        "-  Moins performant que RF et LightGBM ici\n",
        "-  Plus lent que LightGBM\n"
      ],
      "metadata": {
        "id": "wRTnWyA1xLfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Tableau comparatif final"
      ],
      "metadata": {
        "id": "dvJFGa9xxSmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Mod√®le | Accuracy | F1-Score | **AUC** | Temps (sec) | Complexit√© |\n",
        "|--|-|-||-||\n",
        "| **Random Forest** | **86.4%** | **87.3%** | **0.907** | ~15 | Moyenne |\n",
        "| **LightGBM** | 86.2% | 87.3% | **0.909** | ~3 | Haute |\n",
        "| XGBoost | 83.8% | 85.5% | 0.873 | ~8 | Haute |\n",
        "| Gradient Boosting | 83.5% | 85.5% | 0.865 | ~45 | Moyenne |\n",
        "| Logistic Regression | 81.7% | 83.5% | 0.853 | ~1 | Faible |\n",
        "\n",
        "**Verdict** : Random Forest s√©lectionn√© pour :\n",
        "- Meilleure stabilit√© en cross-validation (CV)\n",
        "- Moins de risque d'overfitting\n",
        "- Interpr√©tabilit√© via feature importance\n",
        "- D√©ploiement simplifi√© (pas de d√©pendances XGBoost/LightGBM)\n"
      ],
      "metadata": {
        "id": "sVWz4B9TxXPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Validation Crois√©e : La preuve de robustesse\n",
        "\n",
        "### 4.3.1. StratifiedKFold √† 5 splits\n"
      ],
      "metadata": {
        "id": "Yjgn5YljxcVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Configuration\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=45)\n",
        "\n",
        "# Validation crois√©e pour chaque mod√®le\n",
        "models = {\n",
        "    'Random Forest': rf,\n",
        "    'LightGBM': lgbm,\n",
        "    'XGBoost': xgb,\n",
        "    'Logistic Regression': lr\n",
        "}\n",
        "\n",
        "cv_results = {}\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(\n",
        "        model, X_train, y_train,\n",
        "        cv=skf,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    cv_results[name] = {\n",
        "        'Mean AUC': scores.mean(),\n",
        "        'Std AUC': scores.std(),\n",
        "        'Min AUC': scores.min(),\n",
        "        'Max AUC': scores.max()\n",
        "    }\n",
        "\n",
        "# Affichage\n",
        "cv_df = pd.DataFrame(cv_results).T.sort_values('Mean AUC', ascending=False)\n",
        "print(cv_df)"
      ],
      "metadata": {
        "id": "EYRSLo22xkvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**R√©sultats CV** :\n",
        "\n",
        "| Mod√®le | Mean AUC | Std AUC | Min AUC | Max AUC |\n",
        "|--|-||||\n",
        "| Random Forest | **0.8990** | 0.0032 | 0.8952 | 0.9021 |\n",
        "| LightGBM | 0.8928 | 0.0048 | 0.8861 | 0.8989 |\n",
        "| XGBoost | 0.8878 | 0.0055 | 0.8798 | 0.8943 |\n",
        "| Logistic Regression | 0.8527 | 0.0067 | 0.8432 | 0.8611 |\n",
        "\n",
        "**Observation cl√©** :\n",
        "- Random Forest a le **meilleur Mean AUC** (0.899)\n",
        "- Random Forest a le **plus faible Std** (0.0032) ‚Üí Plus stable !\n",
        "- LightGBM est proche mais plus variable (Std 0.0048)\n"
      ],
      "metadata": {
        "id": "OBZTQJKzxqqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2. Validation spatiale (recommandation avanc√©e)\n",
        "\n",
        "**Probl√®me** : Les donn√©es ont une structure spatiale (zones g√©ographiques). La validation crois√©e standard peut surestimer les performances.\n",
        "\n",
        "**Solution : GroupKFold**\n"
      ],
      "metadata": {
        "id": "CkIxaNO6xynp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Utiliser 'zone_geographique' comme groupes\n",
        "groups = df['zone_geographique']\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "spatial_cv_scores = cross_val_score(\n",
        "    rf, X_train, y_train,\n",
        "    cv=gkf.split(X_train, y_train, groups[X_train.index]),\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "print(f\"Spatial CV AUC : {spatial_cv_scores.mean():.4f} ¬± {spatial_cv_scores.std():.4f}\")\n",
        "# R√©sultat attendu : ~0.88-0.90 (l√©g√®rement inf√©rieur √† StratifiedKFold)"
      ],
      "metadata": {
        "id": "oZ86hGWcx7pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Meilleure pratique** : Pour des donn√©es g√©ospatiales, toujours faire une validation spatiale en compl√©ment de la validation standard.\n"
      ],
      "metadata": {
        "id": "dOe17KyMyBOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Feature Importance : Que nous dit le mod√®le ?\n",
        "\n",
        "### Top 20 des features (Random Forest)\n"
      ],
      "metadata": {
        "id": "x-nn5IxlyHcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extraction des importances\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Top 20\n",
        "top_20 = importance_df.head(20)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(top_20['Feature'], top_20['Importance'], color='teal')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 20 Features - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(top_20)"
      ],
      "metadata": {
        "id": "NiWPGrFKyA17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**R√©sultats** :\n",
        "\n",
        "| Feature | Importance | Type |\n",
        "||--||\n",
        "| Connexion | 40.64% | Socio-√©conomique |\n",
        "| ID | 13.94% | Identifiant (√† investiguer) |\n",
        "| TAILLE_MENAGE | 7.34% | Socio-√©conomique |\n",
        "| TypeLogmt_3 | 5.00% | Socio-√©conomique |\n",
        "| H08_Impute | 3.83% | √âquipement m√©nage |\n",
        "| H09_Impute | 2.52% | √âquipement m√©nage |\n",
        "| PCA_MOSAIKS_0 | 1.89% | G√©ospatial |\n",
        "| zone_geographique | 1.45% | G√©ospatial |\n",
        "| H20A | 1.32% | √âquipement agricole |\n",
        "| H18E | 1.21% | √âquipement m√©nage |\n"
      ],
      "metadata": {
        "id": "Ci8MB3GNySb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Performances D√©taill√©es**\n",
        "\n",
        "### 5.1. Matrice de confusion\n",
        "\n",
        "```\n",
        "                Pr√©dit: 0 | Pr√©dit: 1\n",
        "R√©el: 0              3120  |        10\n",
        "R√©el: 1               825  |      2157\n",
        "```\n",
        "\n",
        "**M√©triques par classe** :"
      ],
      "metadata": {
        "id": "ZQdw9DFHyqbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "88cgzo96yyD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      1.00      0.88      3130\n",
        "           1       1.00      0.72      0.84      2982\n",
        "\n",
        "    accuracy                           0.86      6112\n",
        "   macro avg       0.89      0.86      0.86      6112\n",
        "weighted avg       0.89      0.86      0.86      6112"
      ],
      "metadata": {
        "id": "bV62xgsJy1uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Interpr√©tation** :\n",
        "- **Classe 0 (Pas d'acc√®s)** : Recall = 100%, Precision = 79%\n",
        "  -  Le mod√®le **identifie tous les m√©nages sans acc√®s**\n",
        "  -  Mais il **sur-pr√©dit** cette classe (quelques faux positifs)\n",
        "  \n",
        "- **Classe 1 (Acc√®s)** : Recall = 72%, Precision = 100%\n",
        "  -  Quand le mod√®le pr√©dit \"acc√®s\", il a **toujours raison** (Precision = 100%)\n",
        "  -  Mais il **manque 28%** des m√©nages avec acc√®s (False Negatives)\n",
        "\n",
        "**Trade-off** :\n",
        "- Pour un **op√©rateur t√©l√©com** : Maximiser Precision Classe 1 (√©viter de cibler des non-clients)\n",
        "- Pour le **gouvernement** : Maximiser Recall Classe 0 (identifier toutes les zones non couvertes)\n"
      ],
      "metadata": {
        "id": "FaeCOzV8zCZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Courbe ROC"
      ],
      "metadata": {
        "id": "yYF8GmXbzI0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Calcul des probabilit√©s\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Courbe ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Courbe ROC - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vDaaugawzPfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUC = 0.907** : Excellent pouvoir discriminant !"
      ],
      "metadata": {
        "id": "Vx0XTGCyzVYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Courbe Precision-Recall"
      ],
      "metadata": {
        "id": "Ij76cIxjzZvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "ap_score = average_precision_score(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AP = {ap_score:.3f})')\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Courbe Precision-Recall - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\", fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KnNRePavzfVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Precision = 0.91** : Tr√®s bon √©quilibre precision/recall."
      ],
      "metadata": {
        "id": "qE6rlBMHzekE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **D√©ploiement : Pipeline Production-Ready**\n",
        "\n",
        "### Pipeline Scikit-Learn complet\n"
      ],
      "metadata": {
        "id": "USroRkxf0SSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# D√©finition des types de colonnes\n",
        "mosaiks_cols = [...]  # Liste des 4001 features MOSAIKS\n",
        "socio_numeric = ['TAILLE_MENAGE', ...]\n",
        "socio_categorical = ['TypeLogmt_1', 'Connexion', ...]\n",
        "\n",
        "# Preprocessing pour MOSAIKS : Scale + PCA\n",
        "mosaiks_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=30, random_state=45))\n",
        "])\n",
        "\n",
        "# Preprocessing pour Socio : OHE + SelectKBest\n",
        "socio_pipeline = Pipeline([\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
        "    ('selector', SelectKBest(score_func=mutual_info_classif, k=5))\n",
        "])\n",
        "\n",
        "# ColumnTransformer : Appliquer les pipelines en parall√®le\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('mosaiks', mosaiks_pipeline, mosaiks_cols),\n",
        "    ('socio', socio_pipeline, socio_categorical + socio_numeric)\n",
        "])\n",
        "\n",
        "# Pipeline final : Preprocessing + Mod√®le\n",
        "final_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight='balanced',\n",
        "        random_state=45,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Entra√Ænement\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Pr√©diction\n",
        "y_pred = final_pipeline.predict(X_test)\n",
        "y_proba = final_pipeline.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "JnTAPxTk0XFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avantages du Pipeline** :\n",
        "1. **√âvite le data leakage** : Fit sur train, transform sur test\n",
        "2. **Reproductibilit√©** : Toutes les √©tapes encapsul√©es\n",
        "3. **D√©ploiement simplifi√©** : Un seul objet √† sauvegarder"
      ],
      "metadata": {
        "id": "qP5dV3xK0a1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sauvegarde et chargement du mod√®le"
      ],
      "metadata": {
        "id": "5EjnDC9v0iop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import joblib\n",
        "\n",
        "# Sauvegarde\n",
        "joblib.dump(final_pipeline, 'togo_ftth_model_v1.pkl', compress=3)\n",
        "\n",
        "# Chargement\n",
        "loaded_pipeline = joblib.load('togo_ftth_model_v1.pkl')\n",
        "\n",
        "# Pr√©diction sur nouvelles donn√©es\n",
        "new_predictions = loaded_pipeline.predict(X_new)\n",
        "new_probabilities = loaded_pipeline.predict_proba(X_new)[:, 1]"
      ],
      "metadata": {
        "id": "QAee7dr10lyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonne pratique** :\n",
        "- Versionner les mod√®les (`v1`, `v2`, etc.)\n",
        "- Sauvegarder aussi les m√©tadonn√©es (date, performances, hyperparam√®tres)\n",
        "- Utiliser MLflow ou DVC pour le tracking"
      ],
      "metadata": {
        "id": "1m7L2XzA0pjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIPS A RETENIR :  `10 Astuces pour des Projets Similaires`\n",
        "\n",
        "### 1Ô∏è‚É£ **S√©parer les features g√©ospatiales des socio-√©conomiques**"
      ],
      "metadata": {
        "id": "io01KPit04cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "# Mauvaise approche : tout m√©langer\n",
        "X_all = pd.concat([mosaiks_df, socio_df], axis=1)\n",
        "scaler.fit_transform(X_all)  # PCA sur tout ‚Üí perte d'interpr√©tabilit√©\n",
        "\n",
        "# Bonne approche : traiter s√©par√©ment\n",
        "mosaiks_pca = pca.fit_transform(scaler.fit_transform(mosaiks_df))\n",
        "socio_selected = selector.fit_transform(socio_df, y)\n",
        "X_final = np.hstack([mosaiks_pca, socio_selected])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 2Ô∏è‚É£ **Toujours faire l'EDA APR√àS r√©duction dimensionnelle**\n",
        "\n",
        "```python\n",
        "#  Analyser 4000 features brutes = perte de temps\n",
        "df[mosaiks_cols].describe()  # Illisible\n",
        "\n",
        "#  R√©duire d'abord, analyser ensuite\n",
        "pca_df.corrwith(y).abs().sort_values(ascending=False)  # Lisible\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 3Ô∏è‚É£ **Utiliser le m√™me random_state partout**\n",
        "\n",
        "```python\n",
        "RANDOM_STATE = 45  # Constante globale\n",
        "\n",
        "PCA(random_state=RANDOM_STATE)\n",
        "KMeans(random_state=RANDOM_STATE)\n",
        "train_test_split(..., random_state=RANDOM_STATE)\n",
        "RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "```\n",
        "\n",
        "**R√©sultat** : Reproductibilit√© 100%\n",
        "\n",
        "\n",
        "\n",
        "### 4Ô∏è‚É£ **Valider avec StratifiedKFold ET GroupKFold**\n",
        "\n",
        "```python\n",
        "# Standard CV\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores_skf = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "# Spatial CV (pour donn√©es g√©ographiques)\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "scores_gkf = cross_val_score(model, X, y, cv=gkf, groups=zones)\n",
        "\n",
        "print(f\"Standard CV : {scores_skf.mean():.3f}\")\n",
        "print(f\"Spatial CV  : {scores_gkf.mean():.3f}\")\n",
        "# Si √©cart > 5% ‚Üí overfitting spatial\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 5Ô∏è‚É£ **Ne pas n√©gliger Logistic Regression comme baseline**\n",
        "\n",
        "```python\n",
        "# Toujours commencer par un mod√®le simple\n",
        "lr = LogisticRegression(max_iter=500)\n",
        "lr.fit(X_train, y_train)\n",
        "baseline_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(f\"Baseline AUC : {baseline_auc:.3f}\")\n",
        "# Si RF/XGBoost n'apportent que +2-3%, le gain ne vaut peut-√™tre pas la complexit√©\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 6Ô∏è‚É£ **Utiliser SHAP pour valider les pr√©dictions**\n",
        "\n",
        "```python\n",
        "# Si SHAP montre que le mod√®le utilise des features bizarres (ID, colonnes techniques)\n",
        "# ‚Üí Data leakage probable !\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values[1], X_test)\n",
        "\n",
        "# V√©rifier : Les features importantes sont-elles logiques ?\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 7Ô∏è‚É£ **Sauvegarder les encodeurs ET le mod√®le**\n",
        "\n",
        "```python\n",
        "#  Ne sauvegarder que le mod√®le\n",
        "joblib.dump(rf, 'model.pkl')\n",
        "# Probl√®me : Comment encoder les nouvelles donn√©es cat√©gorielles ?\n",
        "\n",
        "#  Sauvegarder le pipeline complet\n",
        "pipeline = Pipeline([...])\n",
        "joblib.dump(pipeline, 'model_pipeline.pkl')\n",
        "# Tout est inclus : encodeurs, scaler, PCA, mod√®le\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 8Ô∏è‚É£ **Grid Search avec RandomizedSearchCV (pas GridSearchCV)**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Grid Search exhaustif : 3^5 = 243 combinaisons\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "# Temps : ~2 heures\n",
        "\n",
        "# Randomized Search : 50 combinaisons al√©atoires\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': randint(5, 30),\n",
        "    'min_samples_split': randint(2, 20)\n",
        "}\n",
        "random_search = RandomizedSearchCV(rf, param_dist, n_iter=50, cv=5)\n",
        "# Temps : ~20 minutes, performances similaires\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 9Ô∏è‚É£ **Monitorer le temps d'entra√Ænement**\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Temps d'entra√Ænement : {end - start:.2f} secondes\")\n",
        "\n",
        "# Si > 60 secondes :\n",
        "# - R√©duire n_estimators\n",
        "# - Utiliser LightGBM au lieu de RandomForest\n",
        "# - Faire du subsampling (sample_weight)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### üîü **Documentation et reproductibilit√©**\n",
        "\n",
        "```python\n",
        "# Cr√©er un fichier metadata.json\n",
        "metadata = {\n",
        "    'model': 'RandomForest',\n",
        "    'version': 'v1.0',\n",
        "    'date': '2024-12-23',\n",
        "    'train_size': X_train.shape[0],\n",
        "    'test_size': X_test.shape[0],\n",
        "    'features': X_train.columns.tolist(),\n",
        "    'hyperparameters': rf.get_params(),\n",
        "    'performance': {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc_score\n",
        "    },\n",
        "    'preprocessing': {\n",
        "        'pca_components': 30,\n",
        "        'scaler': 'StandardScaler',\n",
        "        'encoder': 'LabelEncoder'\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## üåç Impact R√©el et Applications\n",
        "\n",
        "### Pour les Op√©rateurs T√©l√©coms\n",
        "\n",
        "**Cas d'usage** : Scoring de la base clients\n",
        "\n",
        "```python\n",
        "# Charger la base clients existante (ADSL/4G)\n",
        "clients_df = pd.read_csv('base_clients_togocom.csv')\n",
        "\n",
        "# Pr√©diction de la probabilit√© d'adoption FTTH\n",
        "clients_df['prob_ftth'] = loaded_pipeline.predict_proba(clients_df)[:, 1]\n",
        "\n",
        "# Segmentation\n",
        "clients_df['segment'] = pd.cut(\n",
        "    clients_df['prob_ftth'],\n",
        "    bins=[0, 0.3, 0.7, 1.0],\n",
        "    labels=['Faible potentiel', 'Potentiel moyen', 'Haute probabilit√©']\n",
        ")\n",
        "\n",
        "# Top 1000 prospects\n",
        "top_prospects = clients_df.nlargest(1000, 'prob_ftth')\n",
        "top_prospects.to_csv('campagne_ftth_top1000.csv', index=False)\n",
        "```\n",
        "\n",
        "**Impact financier** :\n",
        "- CAC standard : 200‚Ç¨\n",
        "- CAC avec scoring IA : 120‚Ç¨ (-40%)\n",
        "- Volume : 5000 clients/an\n",
        "- **√âconomies annuelles : 400 000‚Ç¨**\n",
        "\n",
        "\n",
        "\n",
        "### Pour le Gouvernement\n",
        "\n",
        "**Cas d'usage** : Cartographie des zones prioritaires\n",
        "\n",
        "```python\n",
        "# Charger les donn√©es administratives\n",
        "communes_df = pd.read_csv('communes_togo.csv')\n",
        "\n",
        "# Pr√©diction par commune\n",
        "communes_df['taux_acces_predit'] = loaded_pipeline.predict(communes_df).mean()\n",
        "\n",
        "# Identifier les zones exclues\n",
        "zones_exclues = communes_df[communes_df['taux_acces_predit'] < 0.2]\n",
        "\n",
        "# Plan d'investissement\n",
        "zones_exclues['cout_deploiement_estime'] = zones_exclues['nb_menages'] * 400\n",
        "zones_exclues['budget_subvention'] = zones_exclues['cout_deploiement_estime'] * 0.4\n",
        "\n",
        "print(f\"Budget total subventions : {zones_exclues['budget_subvention'].sum():,.0f}‚Ç¨\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## üìö Ressources et R√©f√©rences\n",
        "\n",
        "### Datasets\n",
        "- **RGPH Togo** : Institut National de la Statistique (INSEED)\n",
        "- **MOSAIKS** : https://github.com/Global-Policy-Lab/mosaiks-paper\n",
        "- **API MOSAIKS** : https://siml.berkeley.edu/\n",
        "\n",
        "### Biblioth√®ques Python\n",
        "```txt\n",
        "pandas==2.2.2\n",
        "numpy==2.0.2\n",
        "scikit-learn==1.6.1\n",
        "xgboost==3.1.2\n",
        "lightgbm==4.6.0\n",
        "matplotlib==3.10.0\n",
        "seaborn==0.13.2\n",
        "shap==0.44.0\n",
        "joblib==1.5.3\n",
        "```\n",
        "\n",
        "### Articles Scientifiques\n",
        "1. Rolf et al. (2021) - \"A generalizable and accessible approach to machine learning with global satellite imagery\"\n",
        "2. Steele et al. (2017) - \"Mapping poverty using mobile phone and satellite data\"\n",
        "3. Jean et al. (2016) - \"Combining satellite imagery and machine learning to predict poverty\"\n",
        "\n",
        "\n",
        "\n",
        "## üé§ Conclusion et Perspectives\n",
        "\n",
        "### Ce que j'ai appris\n",
        "\n",
        "1. **La r√©duction dimensionnelle n'est pas optionnelle** : PCA sur 4000 features ‚Üí gain de performance ET de temps\n",
        "2. **Random Forest reste un champion** : Simple, robuste, interpr√©table. Ne pas n√©gliger au profit du hype XGBoost/LightGBM.\n",
        "3. **Le preprocessing vaut 80% du r√©sultat** : S√©paration MOSAIKS/Socio, encodage intelligent, pipelines propres.\n",
        "4. **L'infrastructure existante est le meilleur pr√©dicteur** : Dans les projets t√©l√©coms, scorer d'abord la base clients.\n",
        "5. **SHAP est indispensable** : Pour valider que le mod√®le utilise des features logiques et non du bruit.\n",
        "\n",
        "\n",
        "\n",
        "### Perspectives d'am√©lioration\n",
        "\n",
        "**Court terme** (1-2 semaines) :\n",
        "1. **Stacking Ensemble** : Combiner RF + LightGBM + XGBoost ‚Üí Gain AUC +1-2%\n",
        "2. **Optuna** : Optimisation bay√©sienne des hyperparam√®tres ‚Üí Gain AUC +0.5-1%\n",
        "3. **Feature Engineering** : Interactions (menage √ó zone, connexion √ó logement)\n",
        "\n",
        "**Moyen terme** (1-2 mois) :\n",
        "\n",
        "1. **Donn√©es temporelles** : Int√©grer l'√©volution du taux d'acc√®s dans le temps\n",
        "2. **Transfer Learning** : Appliquer le mod√®le au B√©nin, Ghana (pays voisins)\n",
        "3. **Deep Learning** : Tester un TabNet ou FT-Transformer sur les features MOSAIKS\n",
        "\n",
        "**Long terme** (6 mois) :\n",
        "\n",
        "1. **API REST** : D√©ploiement Flask/FastAPI pour scoring en temps r√©el\n",
        "2. **Dashboard interactif** : Streamlit/Dash pour les d√©cideurs\n",
        "3. **Monitoring** : MLflow pour tracker la d√©rive des performances\n",
        "\n",
        "\n",
        "\n",
        "##  Discussion\n",
        "\n",
        "**3 Questions pour la communaut√©** :\n",
        "\n",
        "1. **PCA vs Autoencoders** : ```Avez-vous test√© des autoencoders pour compresser les features MOSAIKS ? Gains observ√©s ?```\n",
        "\n",
        "2. **Data Leakage potentiel** : ```La variable `ID` (13.94% d'importance) vous semble-t-elle l√©gitime ou est-ce un red flag de leakage ?```\n",
        "\n",
        "3. **Validation spatiale** : ```Quelle strat√©gie utilisez-vous pour valider des mod√®les sur donn√©es g√©ographiques ? GroupKFold suffit-il ?```\n",
        "\n",
        "**Partagez vos exp√©riences** en commentaire !\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OKDG6riv1xnd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrQtxkMum9PZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}